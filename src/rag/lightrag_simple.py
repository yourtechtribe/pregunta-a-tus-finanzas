#!/usr/bin/env python3
"""
Implementaci√≥n simplificada de LightRAG para el proyecto Pregunta-tus-Finanzas
Basada en la documentaci√≥n oficial de HKUDS/LightRAG
"""

import os
import asyncio
import json
from pathlib import Path
from dotenv import load_dotenv
import logging
from typing import List, Dict, Any, Optional

# Cargar variables de entorno
load_dotenv()

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# Importar LightRAG
try:
    from lightrag import LightRAG, QueryParam
    from lightrag.llm.openai import gpt_4o_mini_complete, openai_embed
    from lightrag.utils import EmbeddingFunc
    LIGHTRAG_AVAILABLE = True
    logger.info("‚úÖ LightRAG importado correctamente")
except ImportError as e:
    logger.error(f"‚ùå Error importando LightRAG: {e}")
    logger.error("Instala con: pip install lightrag-hku")
    LIGHTRAG_AVAILABLE = False
    exit(1)


class SimpleFinancialRAG:
    """
    Implementaci√≥n simplificada de LightRAG para datos financieros
    """
    
    def __init__(self, working_dir: str = "rag_financial_knowledge"):
        """
        Inicializa el sistema RAG financiero
        
        Args:
            working_dir: Directorio donde se guardar√° el grafo de conocimiento
        """
        self.working_dir = Path(working_dir)
        self.working_dir.mkdir(parents=True, exist_ok=True)
        self.rag = None
        self.initialized = False
        
        logger.info(f"üìÅ Directorio de trabajo: {self.working_dir}")
        
    async def initialize(self):
        """
        Inicializa LightRAG con la configuraci√≥n correcta seg√∫n la documentaci√≥n
        """
        # Verificar API key
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("‚ùå OPENAI_API_KEY no encontrada en variables de entorno")
        
        logger.info("üîß Inicializando LightRAG...")
        
        try:
            # Inicializar LightRAG con configuraci√≥n b√°sica
            self.rag = LightRAG(
                working_dir=str(self.working_dir),
                llm_model_func=gpt_4o_mini_complete,
                embedding_func=EmbeddingFunc(
                    embedding_dim=1536,
                    max_token_size=8191,
                    func=openai_embed
                )
            )
            
            # Inicializar storages (CR√çTICO seg√∫n la documentaci√≥n)
            logger.info("üì¶ Inicializando storages...")
            await self.rag.initialize_storages()
            
            # Intentar inicializar pipeline status si est√° disponible
            try:
                from lightrag.kg.shared_storage import initialize_pipeline_status
                await initialize_pipeline_status()
                logger.info("‚úÖ Pipeline status inicializado")
            except ImportError:
                logger.warning("‚ö†Ô∏è initialize_pipeline_status no disponible (versi√≥n antigua de LightRAG)")
            
            self.initialized = True
            logger.info("‚úÖ LightRAG inicializado correctamente")
            
        except Exception as e:
            logger.error(f"‚ùå Error inicializando LightRAG: {e}")
            raise
    
    async def insert_financial_data(self, chunks_path: str):
        """
        Inserta los chunks financieros en LightRAG
        
        Args:
            chunks_path: Ruta al archivo JSON con los chunks
        """
        if not self.initialized:
            raise RuntimeError("LightRAG no est√° inicializado. Llama a initialize() primero")
        
        logger.info(f"üì• Cargando chunks desde: {chunks_path}")
        
        # Cargar chunks
        with open(chunks_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Obtener lista de chunks seg√∫n el formato
        if 'documents' in data:
            chunks = data['documents']
        elif 'chunks' in data:
            chunks = data['chunks']
        else:
            chunks = []
        
        logger.info(f"üìä Procesando {len(chunks)} chunks")
        
        # Insertar cada chunk como un documento de texto enriquecido
        inserted = 0
        failed = 0
        
        for i, chunk in enumerate(chunks):
            try:
                # Preparar el texto del chunk
                text = self._prepare_chunk_text(chunk)
                
                if text:
                    # Insertar en LightRAG (usa ainsert para async)
                    await self.rag.ainsert(text)
                    inserted += 1
                    logger.info(f"  ‚úì Chunk {i+1}/{len(chunks)} insertado")
                else:
                    logger.warning(f"  ‚ö†Ô∏è Chunk {i+1} vac√≠o, saltando")
                    
            except Exception as e:
                failed += 1
                logger.error(f"  ‚úó Error en chunk {i+1}: {e}")
        
        logger.info(f"\n‚úÖ Inserci√≥n completada:")
        logger.info(f"  - Insertados: {inserted}")
        logger.info(f"  - Fallidos: {failed}")
        
        return inserted, failed
    
    def _prepare_chunk_text(self, chunk: Dict) -> str:
        """
        Prepara el texto del chunk para inserci√≥n en LightRAG
        Formatea la informaci√≥n para que el LLM pueda extraer entidades correctamente
        
        Args:
            chunk: Diccionario con los datos del chunk
            
        Returns:
            Texto formateado para LightRAG
        """
        # Obtener el contenido principal
        if 'content' in chunk:
            content = chunk['content']
        elif 'prepared_text' in chunk:
            content = chunk['prepared_text']
        elif 'text' in chunk:
            content = chunk['text']
        else:
            return ""
        
        # Obtener metadata
        metadata = chunk.get('metadata', {})
        chunk_type = metadata.get('chunk_type', chunk.get('chunk_type', 'transaction'))
        
        # Construir texto enriquecido para mejor extracci√≥n de entidades
        parts = []
        
        # A√±adir tipo de chunk como contexto
        parts.append(f"[TIPO: {chunk_type}]")
        
        # A√±adir informaci√≥n temporal si existe
        if 'date_range' in metadata:
            date_range = metadata['date_range']
            if 'start' in date_range:
                parts.append(f"Per√≠odo: {date_range['start']} - {date_range.get('end', 'actual')}")
        
        # A√±adir categor√≠a si existe
        if metadata.get('category'):
            parts.append(f"Categor√≠a: {metadata['category']}")
        
        # A√±adir el contenido principal
        parts.append(content)
        
        # A√±adir resumen estad√≠stico si existe
        if 'statistical_summary' in metadata:
            stats = metadata['statistical_summary']
            if 'total' in stats:
                parts.append(f"Total: ‚Ç¨{stats['total']:.2f}")
            if 'count' in stats:
                parts.append(f"N√∫mero de transacciones: {stats['count']}")
            if 'average' in stats:
                parts.append(f"Promedio: ‚Ç¨{stats['average']:.2f}")
        
        return "\n".join(parts)
    
    async def query(self, question: str, mode: str = "hybrid") -> str:
        """
        Realiza una consulta al sistema RAG
        
        Args:
            question: Pregunta en lenguaje natural
            mode: Modo de b√∫squeda ("naive", "local", "global", "hybrid", "mix")
            
        Returns:
            Respuesta generada por el LLM
        """
        if not self.initialized:
            raise RuntimeError("LightRAG no est√° inicializado")
        
        logger.info(f"\nüîç Consulta: {question}")
        logger.info(f"   Modo: {mode}")
        
        try:
            # Configurar par√°metros de consulta
            param = QueryParam(mode=mode)
            
            # Realizar consulta
            response = await self.rag.aquery(question, param)
            
            if response:
                logger.info("‚úÖ Respuesta generada")
                return response
            else:
                logger.warning("‚ö†Ô∏è Sin respuesta")
                return "No pude generar una respuesta para esa pregunta."
                
        except Exception as e:
            logger.error(f"‚ùå Error en consulta: {e}")
            return f"Error procesando la consulta: {str(e)}"
    
    async def batch_query(self, queries: List[str], mode: str = "hybrid") -> List[Dict]:
        """
        Realiza m√∫ltiples consultas en batch
        
        Args:
            queries: Lista de preguntas
            mode: Modo de b√∫squeda
            
        Returns:
            Lista de resultados
        """
        results = []
        
        for query in queries:
            response = await self.query(query, mode)
            results.append({
                "query": query,
                "response": response,
                "mode": mode
            })
        
        return results


async def main():
    """
    Funci√≥n principal para demostrar el uso de LightRAG
    """
    logger.info("\n" + "="*60)
    logger.info("üí∞ SISTEMA RAG FINANCIERO - IMPLEMENTACI√ìN SIMPLE")
    logger.info("="*60)
    
    # Crear instancia del RAG
    rag = SimpleFinancialRAG(working_dir="simple_rag_knowledge")
    
    # Inicializar
    logger.info("\n1Ô∏è‚É£ INICIALIZACI√ìN")
    await rag.initialize()
    
    # Insertar datos
    logger.info("\n2Ô∏è‚É£ INSERCI√ìN DE DATOS")
    chunks_path = "data/embeddings/chunks_with_embeddings_lightrag.json"
    
    if Path(chunks_path).exists():
        inserted, failed = await rag.insert_financial_data(chunks_path)
    else:
        logger.error(f"‚ùå No se encontr√≥: {chunks_path}")
        logger.info("Ejecuta primero: python3 scripts/generate_embeddings.py")
        return
    
    # Realizar consultas de prueba
    logger.info("\n3Ô∏è‚É£ CONSULTAS DE PRUEBA")
    
    test_queries = [
        "¬øCu√°l es mi situaci√≥n financiera general?",
        "¬øEn qu√© categor√≠as gasto m√°s dinero?",
        "¬øCu√°nto gast√© en Groceries?",
        "¬øTengo gastos recurrentes o suscripciones?",
        "¬øCu√°l fue mi mayor gasto del mes?",
        "Dame recomendaciones para ahorrar dinero"
    ]
    
    logger.info("\n" + "-"*60)
    logger.info("PROBANDO DIFERENTES MODOS DE CONSULTA")
    logger.info("-"*60)
    
    # Probar modo hybrid (recomendado)
    for i, query in enumerate(test_queries[:3], 1):
        logger.info(f"\nüìù Consulta {i}: {query}")
        response = await rag.query(query, mode="hybrid")
        
        # Mostrar respuesta (truncada si es muy larga)
        if len(response) > 300:
            preview = response[:300] + "..."
        else:
            preview = response
        
        logger.info(f"ü§ñ Respuesta: {preview}")
    
    # Comparar modos
    logger.info("\n" + "-"*60)
    logger.info("COMPARACI√ìN DE MODOS")
    logger.info("-"*60)
    
    comparison_query = "¬øCu√°les son mis principales gastos?"
    
    for mode in ["naive", "local", "hybrid"]:
        logger.info(f"\nüîç Modo: {mode}")
        response = await rag.query(comparison_query, mode=mode)
        
        if "[no-context]" in response:
            logger.info("   ‚ö†Ô∏è Sin contexto suficiente")
        else:
            preview = response[:200] + "..." if len(response) > 200 else response
            logger.info(f"   ‚Üí {preview}")
    
    logger.info("\n" + "="*60)
    logger.info("‚úÖ DEMO COMPLETADA")
    logger.info("="*60)
    logger.info("\nEl sistema RAG est√° listo para:")
    logger.info("  ‚Ä¢ Responder preguntas sobre tus finanzas")
    logger.info("  ‚Ä¢ Generar an√°lisis con IA")
    logger.info("  ‚Ä¢ Proporcionar recomendaciones personalizadas")
    logger.info("\nüí° Nota: Con m√°s datos, las respuestas ser√°n m√°s precisas")


if __name__ == "__main__":
    # Ejecutar funci√≥n as√≠ncrona
    asyncio.run(main())